import os
from llama_cpp import Llama
import json
from pathlib import Path

MODEL_FILE = "openthaigpt1.5-7B-instruct-Q4KM.gguf"
BASE_DIR = Path(__file__).resolve().parent

# ‚úÖ ‡∏•‡πá‡∏≠‡∏Å‡∏û‡∏≤‡∏ò‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
MODEL_PATH = BASE_DIR / "llama.cpp" / "build" / "models" / "openthaigpt" / MODEL_FILE

# (‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô) ‡∏ñ‡πâ‡∏≤‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏≠‡∏¢‡∏≤‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡πâ‡∏á env ‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏ô
_env = os.environ.get("LLM_GGUF_PATH")
if _env and Path(_env).exists():
    MODEL_PATH = Path(_env)

if not MODEL_PATH.exists():
    raise ValueError(
        f"Model path does not exist: {MODEL_PATH}")

MODEL_PATH_STR = os.fspath(MODEL_PATH)
llm = Llama(
    model_path=MODEL_PATH_STR,
    n_ctx=32768,
    n_threads=8,
    n_batch=512,
    use_mlock=True,
    rope_freq_base=1000000.0,
    chat_format="qwen",
    verbose=False,
    escape=True,
)
 
def ask_llm_raw(prompt: str) -> str:
    output = llm.create_chat_completion(
        messages=[
            {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏ã‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ï‡∏¢‡πå‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ'"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.9,
        top_p=0.7,
        top_k=40,
        max_tokens=512,
        repeat_penalty=1.1,
    )
    return output['choices'][0]['message']['content']

import json

def ask_llm_plan_json(text: str) -> dict:
    prompt = f"""
‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: "{text}"

‡∏à‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö backend ‡πÇ‡∏î‡∏¢‡∏°‡∏µ intent ‡πÄ‡∏õ‡πá‡∏ô "Plan" ‡πÅ‡∏•‡∏∞ decorated_input ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
- goal: ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏° ‡πÄ‡∏ä‡πà‡∏ô "‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà"
- location: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà ‡πÄ‡∏ä‡πà‡∏ô "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà"
- duration_days: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô ‡πÄ‡∏ä‡πà‡∏ô 5
- detail: ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡πÄ‡∏ä‡πà‡∏ô "‡∏á‡∏ö‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 5000, ‡∏Ç‡∏≠‡∏ô‡∏±‡πà‡∏á‡∏£‡∏ñ‡∏ó‡∏±‡∏ß‡∏£‡πå" (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô "")

‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:

{{
  "intent": "Plan",
  "decorated_input": {{
    "goal": "...",
    "location": "...",
    "duration_days": ...,
    "detail": "..."
  }}
}}

‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏≠‡∏∑‡πà‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ markdown (` ``` `) ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö' ‡∏ï‡∏≠‡∏ö JSON ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
"""

    response = ask_llm_raw(prompt)

    # üîß ‡∏•‡πâ‡∏≤‡∏á prefix/suffix ‡∏ó‡∏µ‡πà model ‡∏ä‡∏≠‡∏ö‡πÅ‡∏ñ
    cleaned = (
        response.strip()
        .removeprefix("```json")
        .removeprefix("```")
        .replace("‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö", "")
        .strip()
        .removesuffix("```")
    )

    return json.loads(cleaned)
