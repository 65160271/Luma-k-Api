import os
import sys
import requests
from bs4 import BeautifulSoup
from serpapi import GoogleSearch
from dotenv import load_dotenv
from llama_cpp import Llama
from pydantic import BaseModel

# === Models ===
class PromptRequest(BaseModel):
    prompt: str
    api_key: str = None

# === à¹‚à¸«à¸¥à¸”à¸„à¹ˆà¸² .env ===
load_dotenv()

# === Trigger à¸—à¸µà¹ˆà¸šà¸­à¸à¸§à¹ˆà¸² LLM à¸•à¸­à¸šà¹à¸šà¸šà¹„à¸¡à¹ˆà¸£à¸¹à¹‰ ===
FALLBACK_TRIGGERS = [
    "à¸‰à¸±à¸™à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ªà¸ à¸²à¸žà¸­à¸²à¸à¸²à¸¨",
    "à¸‰à¸±à¸™à¹€à¸›à¹‡à¸™à¹‚à¸›à¸£à¹à¸à¸£à¸¡ AI à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¸­à¸´à¸™à¹€à¸—à¸­à¸£à¹Œà¹€à¸™à¹‡à¸•",
    "à¸„à¸¸à¸“à¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ªà¸ à¸²à¸žà¸­à¸²à¸à¸²à¸¨à¹„à¸”à¹‰à¸ˆà¸²à¸",
    "à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥",
    "à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸­à¸š"
]

# === à¹‚à¸«à¸¥à¸” Llama model (à¹à¸—à¸™ llama-cli) ===
llm = Llama(
    model_path="./llama.cpp/build/models/openthaigpt/openthaigpt1.5-7B-instruct-Q4KM.gguf",
    n_ctx=32768,
    n_threads=8,
    n_batch=512,
    use_mlock=True,
    rope_freq_base=1000000.0,        # à¸•à¸£à¸‡à¸à¸±à¸š metadata
    chat_format="qwen",              # à¹ƒà¸Šà¹‰ template à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š CLI
    verbose=False,                   # à¹ƒà¸«à¹‰à¹€à¸‡à¸µà¸¢à¸šà¹€à¸«à¸¡à¸·à¸­à¸™ CLI
    escape=True,                     # à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸šà¸±à¹Šà¸ input à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸„à¹‰à¸²à¸‡
)

def ask_llm_raw(prompt: str) -> str:
    output = llm.create_chat_completion(
        messages=[
            {"role": "system", "content": "à¸„à¸¸à¸“à¸„à¸·à¸­à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸—à¸µà¹ˆà¸‰à¸¥à¸²à¸”à¹à¸¥à¸°à¸‹à¸·à¹ˆà¸­à¸ªà¸±à¸•à¸¢à¹Œ"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.7,
        top_p=0.95,
        top_k=40,
        max_tokens=512,
        repeat_penalty=1.1,
    )
    return output['choices'][0]['message']['content']

# === à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸² LLM à¸•à¸­à¸šà¹à¸šà¸š fallback à¸«à¸£à¸·à¸­à¸¡à¸±à¹ˆà¸§ ===
def is_llm_uncertain(text: str) -> bool:
    return any(phrase in text for phrase in FALLBACK_TRIGGERS)

# === à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸ˆà¸²à¸à¹€à¸§à¹‡à¸š ===
def scrape_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        return soup.get_text(separator=" ", strip=True)
    except Exception as e:
        print(f"âŒ Error scraping {url}: {e}")
        return ""

# === à¹ƒà¸Šà¹‰ Google Search API ===
def search_google(query, api_key, num_results=1):
    try:
        api_key = os.environ.get("SERPAPI_API_KEY")
        if not api_key:
            raise ValueError("âŒ à¹„à¸¡à¹ˆà¸žà¸š SERPAPI_API_KEY à¹ƒà¸™ environment")
    
        search = GoogleSearch({
            "q": query,
            "api_key": api_key,
            "num": num_results,
            "hl": "th",
            "gl": "th"
        })
        results = search.get_dict()
        organic = results.get("organic_results", [])
        return [r.get("link") for r in organic[:num_results] if "link" in r]
    except Exception as e:
        print(f"âŒ Error searching Google: {e}")
        return []

# === main logic ===
def ask_with_cli_and_fallback(prompt, api_key):
    print(f"ðŸ“¨ user: {prompt}")
    answer = ask_llm_raw(prompt)

    if not is_llm_uncertain(answer):
        return answer

    print("ðŸŒ fallback: à¹„à¸¡à¹ˆà¸¡à¸±à¹ˆà¸™à¹ƒà¸ˆ â†’ Google")
    urls = search_google(prompt, api_key, num_results=1)
    print("ðŸŒ URLs à¸—à¸µà¹ˆà¹„à¸”à¹‰:", urls)

    if not urls:
        return answer + "\n(à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸„à¹‰à¸™à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡à¹„à¸”à¹‰à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰)"

    page_text = scrape_text(urls[0])
    if not page_text:
        return answer + "\n(à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¹€à¸§à¹‡à¸šà¹„à¸‹à¸•à¹Œà¹„à¸”à¹‰)"

    refined_prompt = f"à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸™à¸µà¹‰ à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰à¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸±à¹‰à¸™à¸à¸£à¸°à¸Šà¸±à¸š:\n\n{page_text[:4000]}\n\nà¸„à¸³à¸–à¸²à¸¡: {prompt}"
    final_answer = ask_llm_raw(refined_prompt)
    return final_answer

if __name__ == "__main__":
    if not os.environ.get("SERPAPI_API_KEY"):
        print("âŒ à¹„à¸¡à¹ˆà¸žà¸š SERPAPI_API_KEY à¹ƒà¸™ environment")
        sys.exit(1)

    print("à¸žà¸´à¸¡à¸žà¹Œà¸„à¸³à¸–à¸²à¸¡ à¸«à¸£à¸·à¸­ 'exit' à¹€à¸žà¸·à¹ˆà¸­à¸­à¸­à¸")
    while True:
        user_input = input("prompt> ").strip()
        if user_input.lower() in ["exit", "quit"]:
            break
        result = ask_with_cli_and_fallback(user_input, os.environ.get("SERPAPI_API_KEY"))
        print("âœ… à¸„à¸³à¸•à¸­à¸š:", result)
